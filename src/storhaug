#!/bin/bash

# STORHAUG: High-Availability Storage Server with Pacemaker
#
# Copyright (c) 2015 Kaleb S. Keithley <kkeithle@redhat.com>
# Copyright (c) 2015 Meghana Madhusudhan <mmadhusu@redhat.com>
# Copyright (c) 2015 Jose A. Rivera <jarrpa@redhat.com>
# Copyright (c) 2015 Red Hat Inc.
#   All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of version 2 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it would be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
# Further, this software is distributed without any warranty that it is
# free of the rightful claim of any third person regarding infringement
# or the like.  Any license provided herein, whether implied or
# otherwise, applies only to this software file.  Patent licenses, if
# any, provided herein do not apply to combinations of this program with
# other software, or any other product whatsoever.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write the Free Software Foundation,
# Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
#

HA_NUM_SERVERS=0
HA_SERVERS=""
SYS_CONFDIR="/etc"
HA_CONF="${SYS_CONFDIR}/sysconfig/storhaug.conf"
HA_CONF_INCDIR="${SYS_CONFDIR}/sysconfig/storhaug.d/"
HA_MNT_DIR="/var/run/gluster"
HA_SMB_MNT_DIR="lock"
HA_NFS_MNT_DIR="state"
STORAGE_SERVERS=""
STORAGE_NUM_SERVERS=0
DETERMINISTIC_FAILOVER=false

### Utility functions

usage()
{
    echo -e "Usage: `basename "$0"` [<OPTIONS>] <COMMAND> [<ARGUMENTS>]"
    echo -e "Manage a storhaug high-availability (HA) storage cluster."
    echo -e "\nGlobal OPTIONS:"
    echo -e "  -h, --help            Output this useful help message"
    echo -e "\nCOMMANDS:"
    echo -e "  status                Check the status of the cluster"
    echo -e "  setup                 Setup a new cluster"
    echo -e "  teardown              Teardown an existing cluster"
    echo -e "  add                   Add a node to the cluster"
    echo -e "  delete, remove        Remove a node from the cluster"
    echo -e "\nCommand ARGUMENTS:"
    echo -e "  add <NODE>            Add hostname NODE to the cluster"
    echo -e "  remove <NODE>         Remove hostname NODE from the cluster"
    echo -e "  delete <NODE>         Synonym for 'remove'"
    echo -e "\n\nConfiguration is read from the following locations:"
    echo -e "  ${HA_CONF}"
    echo -e "  ${HA_CONF_INCDIR}*.conf"
}

parsebool()
{
    case $(eval echo \${${1}}) in
        TRUE | True | true | YES | Yes | yes) declare "${1}"=true ;;
        FALSE | False | false | NO | No | no) declare "${1}"=false ;;
        *) storlog "ERR" "Couldn't parse boolean: ${1}=$(eval echo \${${1}})" ;;
    esac
}

storlog()
{
    LEVEL=$1; shift
    case $LEVEL in
        ERR|ERROR)
        echo "ERROR: $1" >&2
        logger --tag="storhaug" -p "daemon.err" "$1"
        rm -rf $HA_CONF_secdir
        exit 1
        ;;
        WARN|WARNING)
        echo "WARNING: $1"
        logger --tag="storhaug" -p "daemon.warn" "$1"
        ;;
        INFO)
        echo "$1"
        logger --tag="storhaug" -p "daemon.info" "$1"
        ;;
        DEBUG)
        logger --tag="storhaug" -p "daemon.debug" "$1"
        ;;
    esac
}

### Cluster functions

check_cluster_exists()
{
    local name=${1}
    local cluster_name=""

    if [ -e /var/run/corosync.pid ]; then
        cluster_name=$(pcs status | grep "Cluster name:" | cut -d ' ' -f 3)
        if [ ${cluster_name} -a ${cluster_name} = ${name} ]; then
            storlog "ERR" "Cluster $name already exists, exiting"
        fi
    fi
}

determine_servers()
{
    local cmd=${1}
    local num_servers=0
    local tmp_ifs=${IFS}
    local ha_servers=""

    if [[ "X${cmd}X" != "XsetupX" ]]; then
        ha_servers=$(pcs status | grep "Online:" | grep -o '\[.*\]' | sed -e 's/\[//' | sed -e 's/\]//')
        IFS=$' '
        for server in ${ha_servers} ; do
            num_servers=$(expr ${num_servers} + 1)
        done
        IFS=${tmp_ifs}
        HA_NUM_SERVERS=${num_servers}
        HA_SERVERS="${ha_servers}"
        # TODO: Determine storage and vip servers from pcs status
        if [[ "x${STORAGE_NODES}" != "x" ]]; then
            STORAGE_SERVERS="${STORAGE_NODES//,/ }"
            STORAGE_NUM_SERVERS=$(wc -w <<< "${STORAGE_SERVERS}")
        fi
        if [[ "x${VIP_NODES}" != "x" ]]; then
            VIP_SERVERS="${VIP_NODES//,/ }"
        fi
    else
        IFS=$','
        for server in ${HA_CLUSTER_NODES} ; do
            num_servers=$(expr ${num_servers} + 1)
        done
        IFS=${tmp_ifs}
        HA_NUM_SERVERS=${num_servers}
        HA_SERVERS="${HA_CLUSTER_NODES//,/ }"
        if [[ "x${STORAGE_NODES}" != "x" ]]; then
            STORAGE_SERVERS="${STORAGE_NODES//,/ }"
            STORAGE_NUM_SERVERS=$(wc -w <<< "${STORAGE_SERVERS}")
        fi
        if [[ "x${VIP_NODES}" != "x" ]]; then
            VIP_SERVERS="${VIP_NODES//,/ }"
        fi
    fi
}

setup_cluster()
{
    local unclean=""

    storlog "INFO" "Setting up cluster ${HA_NAME} on the following servers: ${servers}"

    pcs cluster auth ${HA_SERVERS} -u hacluster -p ${HA_PASSWORD} --force
    pcs cluster setup --force --name ${HA_NAME} ${HA_SERVERS} || storlog "ERR" "Failed to setup cluster ${HA_NAME}"
    pcs cluster start --all || storlog "ERR" "Failed to start cluster ${HA_NAME}"

    sleep 3
    unclean=$(pcs status | grep -u "UNCLEAN")
    while [[ "${unclean}X" = "UNCLEANX" ]]; do
         sleep 1
         unclean=$(pcs status | grep -u "UNCLEAN")
    done
    sleep 1

    local tmp_ifs=${IFS}
    IFS=$' '
    for server in ${STORAGE_SERVERS:-$HA_SERVERS} ; do
        pcs property set --node $server role=storage || \
          storlog "WARN" "Failed: pcs property set --node $server role=storage"
    done
    IFS=${tmp_ifs}

    if [ ${HA_NUM_SERVERS} -lt 3 ]; then
        pcs property set no-quorum-policy=ignore || \
          storlog "WARN" "Failed: pcs property set no-quorum-policy=ignore"
    fi
    pcs property set stonith-enabled=false || storlog "WARN" "Failed: pcs property set stonith-enabled=false"
}

teardown_cluster()
{
    local name=${1}

    storlog "INFO" "Tearing down cluster $name"

    for server in ${HA_SERVERS} ; do
        if [[ ${HA_CLUSTER_NODES} != *${server}* ]]; then
            storlog "INFO" "${server} is not in config, removing"

            pcs cluster stop ${server} || storlog "WARN" "Failed: pcs cluster stop ${server}"

            pcs cluster node remove ${server} || storlog "WARN" "Failed: pcs cluster node remove ${server}"
        fi
    done

# BZ 1193433 - pcs doesn't reload cluster.conf after modification
# after teardown completes, a subsequent setup will appear to have
# 'remembered' the deleted node. You can work around this by
# issuing another `pcs cluster node remove $node`,
# `crm_node -f -R $server`, or
# `cibadmin --delete --xml-text '<node id="$server"
# uname="$server"/>'

    pcs cluster stop --all || storlog "WARN" "Failed to stop cluster ${name}"

    pcs cluster destroy || storlog "ERR" "Failed to destroy cluster ${name}"
}

### Resource functions

do_create_virt_ip_constraints()
{
    local cibfile=${1}; shift
    local ipcount=${1}; shift
    local primary=${1}; shift
    local weight="1000"

    # A set of location constraints to set the prefered order
    # for where a VIP should move
    while [[ ${1} ]]; do
        pcs -f ${cibfile} constraint location vip${ipcount} prefers ${1}=${weight} || \
          storlog "WARN" "Failed: pcs constraint location vip${ipcount} prefers ${1}=${weight}"
        weight=$(expr ${weight} + 1000)
        shift
    done

    # Set the highest preference for the VIP to its primary node
    pcs -f ${cibfile} constraint location vip${ipcount} prefers ${primary}=${weight} || \
      storlog "WARN" "Failed: pcs constraint location vip${ipcount} prefers ${primary}=${weight}"
}

wrap_create_virt_ip_constraints()
{
    local cibfile=${1}; shift
    local ipcount=${1}; shift
    local srvcount=${ipcount}
    local primary=""
    local head=""
    local tail=""

    # build a list of failover peers, e.g. for a four node cluster, for node1,
    # the result is "node2 node3 node4"; for node2, "node3 node4 node1"
    # and so on.
    read -r -a servers <<< "${VIP_SERVERS:-STORAGE_SERVERS}"
    while [ ${srvcount} -gt ${STORAGE_NUM_SERVERS} ]; do
        srvcount=$((srvcount - STORAGE_NUM_SERVERS))
    done
    primary=${servers[${srvcount}-1]}
    if [ ${STORAGE_NUM_SERVERS} -gt 1 ]; then
        head=${servers[@]:${srvcount}-${STORAGE_NUM_SERVERS}-1}
        tail=${servers[@]:${srvcount}}
    fi

    do_create_virt_ip_constraints ${cibfile} ${ipcount} ${primary} ${tail} ${head}
}

create_virt_ip_constraints()
{
    local cibfile=${1}; shift
    local ipcount=0

    for ip in ${HA_VIPS}; do
        ((ipcount++))
        wrap_create_virt_ip_constraints ${cibfile} ${ipcount}
        shift
    done
}

setup_create_resources()
{
    local cibfile=$(mktemp --tmpdir=$HA_CONF_secdir)

    pcs cluster cib ${cibfile}

    # Shared volumes
    mkdir -p "${HA_MNT_DIR}/${HA_SMB_MNT_DIR}"
    pcs -f ${cibfile} resource create ctdb_lock ocf:heartbeat:Filesystem \
        params \
            device="localhost:/${HA_SMB_VOL}" \
            directory="${HA_MNT_DIR}/${HA_SMB_MNT_DIR}" \
            fstype="glusterfs" \
            options="_netdev,defaults,direct-io-mode=enable,transport=tcp,xlator-option=*client*.ping-timeout=10" \
        --clone ctdb_lock-clone ctdb_lock meta interleave="true" clone-max="${STORAGE_NUM_SERVERS}"

    pcs -f ${cibfile} constraint location ctdb_lock-clone rule resource-discovery=exclusive score=0 role eq storage

#    mkdir -p /gluster/state
#    pcs -f ${cibfile} resource create ganesha_state ocf:heartbeat:Filesystem \
#        params \
#            device="localhost:/$HA_NFS_VOL" \
#            directory="/gluster/state" \
#            fstype="glusterfs" \
#                       options="_netdev,defaults,direct-io-mode=enable,transport=tcp,xlator-option=*client*.ping-timeout=10" \
#               --clone ganesha_state-clone ganesha_state meta interleave="true" clone-max="${STORAGE_NUM_SERVERS}"

    pcs cluster cib-push ${cibfile} || storlog "ERR" "Failed to create filesystem resources."

    # CTDB
    pcs -f ${cibfile} resource create ctdb ocf:heartbeat:CTDB \
        params \
            ctdb_recovery_lock="${HA_MNT_DIR}/${HA_SMB_MNT_DIR}/lockfile" \
            ctdb_socket="/var/run/ctdb/ctdbd.socket" \
            ctdb_manages_winbind="no" \
            ctdb_manages_samba="no" \
            ctdb_logfile="/var/log/log.ctdb" \
        op monitor interval="10" timeout="30" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100" \
        --clone ctdb-clone ctdb meta interleave="true" globally-unique="false" clone-max="${STORAGE_NUM_SERVERS}"

    # CTDB: We need our shared recovery lock file
    pcs -f ${cibfile} constraint colocation add ctdb-clone with ctdb_lock-clone INFINITY
    pcs -f ${cibfile} constraint order ctdb_lock-clone then ctdb-clone INFINITY

    # Samba
    pcs -f ${cibfile} resource create nmb systemd:nmb \
        op start timeout="60" interval="0" \
        op stop timeout="60" interval="0" \
        op monitor interval="60" timeout="60"
    pcs -f ${cibfile} resource create smb systemd:smb \
        op start timeout="60" interval="0" \
        op stop timeout="60" interval="0" \
        op monitor interval="60" timeout="60"
    pcs -f ${cibfile} resource group add samba-group nmb smb
    pcs -f ${cibfile} resource clone samba-group meta interleave="true" clone-max="${STORAGE_NUM_SERVERS}"

    # Samba: We need CTDB
    pcs -f ${cibfile} constraint colocation add samba-group-clone with ctdb-clone INFINITY
    pcs -f ${cibfile} constraint order ctdb-clone then samba-group-clone INFINITY

    # Ganesha
#    pcs -f ${cibfile} resource create ganesha ganesha \
#        params \
#            config="/etc/glusterfs-ganesha/nfs-ganesha.conf" \
#        --clone ganesha-clone ganesha meta interleave="true" \
#                                           globally-unique="false" \
#                                           notify="true"
#
#    # Ganesha: We need our shared state FS
#    pcs -f ${cibfile} constraint colocation add ganesha-clone with ganesha_state-clone INFINITY
#    pcs -f ${cibfile} constraint order ganesha_state-clone then ganesha-clone INFINITY
#
#    pcs cluster cib-push ${cibfile} || storlog "ERR" "Failed to create service resources."

    # Virtual IPs
    local ipcount=0
    for ip in ${HA_VIPS}; do
        ((ipcount++))
        pcs -f ${cibfile} resource create vip${ipcount} ocf:heartbeat:IPaddr2 \
            params \
                ip=${ip} \
                flush_routes="true" \
            op monitor interval=60s \
            meta resource-stickiness="0"

        pcs -f ${cibfile} constraint location vip${ipcount} rule resource-discovery=exclusive score=0 role eq storage

#        pcs -f ${cibfile} resource create vip${ipcount}_trigger ocf:heartbeat:ganesha_trigger \
#            params \
#                ip=${ip} \
#            meta resource-stickiness="0"
#
#        pcs -f ${cibfile} constraint colocation add vip${ipcount}_trigger with vip${ipcount} INFINITY
#        pcs -f ${cibfile} constraint order vip${ipcount} then vip${ipcount}_trigger
    done

    if [[ ${DETERMINISTIC_FAILOVER} == true ]]; then
        create_virt_ip_constraints ${cibfile}
    fi

    pcs cluster cib-push ${cibfile} || storlog "ERR" "Failed to create virtual IP resources."

    rm -f ${cibfile}
}

### Shared state

setup_state_volume()
{
    local mnt=$(mktemp -d --tmpdir=$HA_CONF_secdir)
    local longname=""
    local shortname=""
    local dname=""

    mount -t glusterfs ${HA_SERVER}:/${HA_NFS_VOL} ${mnt}

    longname=$(hostname)
    dname=${longname#$(hostname -s)}

    while [[ ${1} ]]; do
        mkdir -p ${mnt}/${1}${dname}/nfs/ganesha/v4recov
        mkdir -p ${mnt}/${1}${dname}/nfs/ganesha/v4old
        mkdir -p ${mnt}/${1}${dname}/nfs/statd/sm
        mkdir -p ${mnt}/${1}${dname}/nfs/statd/sm.bak
        mkdir -p ${mnt}/${1}${dname}/nfs/statd/state
        touch ${mnt}/${1}${dname}/nfs/state
        for server in ${HA_SERVERS} ; do
            if [ ${server} != ${1}${dname} ]; then
                ln -s ${mnt}/${server}/nfs/ganesha ${mnt}/${1}${dname}/nfs/ganesha/${server}
                ln -s ${mnt}/${server}/nfs/statd ${mnt}/${1}${dname}/nfs/statd/${server}
            fi
        done
        shift
    done

    umount ${mnt}
    rmdir ${mnt}
}

### Mainline

cmd=${1}; shift
if [[ ${cmd} == *help || ${cmd} == "-h" ]]; then
    usage
    exit 0
elif [[ ${cmd} == *status ]]; then
    exec pcs status
    exit 0
fi
node=""
vip=""

HA_CONF_secdir=$(mktemp -d --tmpdir "$(basename $0).XXXXXXXXXX")
HA_CONF_sec="$HA_CONF_secdir/sec.conf"

# Filter all config files into secure format
egrep '^#|^[^ ]*=[^;&]*'  "$HA_CONF" > "$HA_CONF_sec"
for conffile in `ls $HA_CONF_INCDIR 2>/dev/null`; do
    egrep '^#|^[^ ]*=[^;&]*'  "$HA_CONF_INCDIR/$conffile" >> "$HA_CONF_sec"
done

# Source/load the config
. $HA_CONF_sec

parsebool "DETERMINISTIC_FAILOVER"

case "${cmd}" in
    setup | --setup)
        storlog "INFO" "Setting up ${HA_NAME}"
        check_cluster_exists ${HA_NAME}
        determine_servers "setup"

        if [ ${HA_NUM_SERVERS} -gt 1 ]; then
#            setup_state_volume ${HA_SERVERS}
            setup_cluster
            setup_create_resources ${HA_SERVERS}
        else
            storlog "ERR" "Insufficient servers for HA, aborting"
        fi
        ;;
    teardown | --teardown)
        storlog "INFO" "Tearing down ${HA_NAME}"
        determine_servers "teardown"
        teardown_cluster ${HA_NAME}
        ;;
    add | --add)
        node=${1}; shift
        storlog "INFO" "Adding ${node} to ${HA_NAME}"
        pcs cluster node add ${node} || storlog "WARN" "Failed: pcs cluster node add ${node}"
        pcs cluster start ${node} || storlog "ERR" "Failed: pcs cluster start ${add_node}"
        ;;
    delete | --delete | remove | --remove)
        node=${1}; shift
        logger "deleting ${node} from ${HA_NAME}"
        pcs cluster node remove ${node} || storlog "WARN" "Failed: pcs cluster node remove ${node}"
        ;;
    *)
        storlog "ERR" "Unknown argument: ${cmd}"
        ;;
esac

rm -rf $HA_CONF_secdir
